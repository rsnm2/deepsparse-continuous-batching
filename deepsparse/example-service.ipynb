{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a213aea43d5f42e8a885e6d481a9c094",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# download a model\n",
    "from huggingface_hub import snapshot_download\n",
    "MODEL_PATH = snapshot_download(repo_id=\"mgoin/TinyStories-33M-quant-deepsparse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.json\t\tmodel.onnx\t\t tokenizer_config.json\n",
      "generation_config.json\tmodel-orig.onnx\t\t tokenizer.json\n",
      "merges.txt\t\tspecial_tokens_map.json  vocab.json\n"
     ]
    }
   ],
   "source": [
    "!ls {MODEL_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "Using pad_token, but it is not set yet.\n",
      "2023-10-17 22:44:55 deepsparse.utils.onnx INFO     Overwriting in-place the input shapes of the transformer model at /home/rshaw/.cache/huggingface/hub/models--mgoin--TinyStories-33M-quant-deepsparse/snapshots/6d30653d6fd728a5b8121a2e6801408c79c3c179/model.onnx\n",
      "DeepSparse, Copyright 2021-present / Neuralmagic, Inc. version: 1.6.0.20231012 COMMUNITY | (ecee26fb) (release) (optimized) (system=avx2, binary=avx2)\n",
      "2023-10-17 22:44:58 deepsparse.utils.onnx INFO     Overwriting in-place the input shapes of the transformer model at /home/rshaw/.cache/huggingface/hub/models--mgoin--TinyStories-33M-quant-deepsparse/snapshots/6d30653d6fd728a5b8121a2e6801408c79c3c179/model.onnx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deepsparse.engine.Engine:\n",
      "\tonnx_file_path: /home/rshaw/.cache/huggingface/hub/models--mgoin--TinyStories-33M-quant-deepsparse/snapshots/6d30653d6fd728a5b8121a2e6801408c79c3c179/model.onnx\n",
      "\tbatch_size: 1\n",
      "\tnum_cores: 24\n",
      "\tnum_streams: 1\n",
      "\tscheduler: Scheduler.default\n",
      "\tfraction_of_supported_ops: 1.0\n",
      "\tcpu_avx_type: avx2\n",
      "\tcpu_vnni: False\n",
      "deepsparse.engine.Engine:\n",
      "\tonnx_file_path: /home/rshaw/.cache/huggingface/hub/models--mgoin--TinyStories-33M-quant-deepsparse/snapshots/6d30653d6fd728a5b8121a2e6801408c79c3c179/model.onnx\n",
      "\tbatch_size: 1\n",
      "\tnum_cores: 24\n",
      "\tnum_streams: 1\n",
      "\tscheduler: Scheduler.default\n",
      "\tfraction_of_supported_ops: 1.0\n",
      "\tcpu_avx_type: avx2\n",
      "\tcpu_vnni: False\n"
     ]
    }
   ],
   "source": [
    "from service.causal_lm import DeepSparseCausalLM\n",
    "from service.service import DeepSparseService\n",
    "\n",
    "# setup service\n",
    "service = DeepSparseService(\n",
    "    model = DeepSparseCausalLM(\n",
    "        model_path=f\"{MODEL_PATH}/model.onnx\",\n",
    "        tokenizer_path=MODEL_PATH\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import CachedBatch, Batch, Generation, GenerateRequest, Request, GenerationParameters, GenerateRequestInputs\n",
    "\n",
    "# setup inputs\n",
    "prompts = [\n",
    "    \"Pricess Peach jumped from the balcony and\",\n",
    "    \"Mario and Luigi ran out of the store and\",\n",
    "    \"Bowser took out the flamethrower and\",\n",
    "    \"Wario shaved his mustache and\",\n",
    "    \"Toad made a funny sound and\",\n",
    "]\n",
    "\n",
    "\n",
    "### ----- IMPORTANT ------\n",
    "# NOTE: this controls how many decodes will be run for each request\n",
    "max_new_tokens_list = [30, 6, 17, 9, 10]\n",
    "\n",
    "requests = [\n",
    "    Request(\n",
    "        id=idx,\n",
    "        inputs=prompt,\n",
    "        generation_parameters=GenerationParameters(max_new_tokens=max_new_tokens),\n",
    "    ) for idx, (prompt, max_new_tokens) in enumerate(zip(prompts, max_new_tokens_list))\n",
    "]\n",
    "\n",
    "batches = [\n",
    "    Batch(\n",
    "        id=idx,\n",
    "        requests=[request]\n",
    "    ) for idx, request in enumerate(requests)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_DECODES = 5\n",
    "\n",
    "def do_decode(service, cached_batch_list, text_list):\n",
    "    generations, next_batch = service.Decode(cached_batch_list)\n",
    "\n",
    "    if next_batch is None:\n",
    "        cached_batch_list = []\n",
    "        service.ClearCache()\n",
    "        \n",
    "        print(\"-------- COMPLETED GENERATION --------\")\n",
    "        print(f\"id = {generations[0].request_id}: {text_list[generations[0].request_id]}\")\n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "        return cached_batch_list, text_list\n",
    "\n",
    "    cached_batch_list = [next_batch]\n",
    "    for generation in generations:\n",
    "        if generation.stopped:\n",
    "            batch_id = cached_batch_list[0].batch_id\n",
    "            active_request_ids = [\n",
    "                request_id for request_id in cached_batch_list[0].request_ids if request_id != generation.request_id\n",
    "            ]\n",
    "\n",
    "            service.FilterBatch(\n",
    "                batch_id=batch_id, request_ids=active_request_ids\n",
    "            )\n",
    "\n",
    "            print(\"-------- COMPLETED GENERATION --------\")\n",
    "            print(f\"id = {generation.request_id}: {text_list[generation.request_id]}\")\n",
    "            print(\"\\n\\n\")\n",
    "\n",
    "        text_list[generation.request_id] += generation.token\n",
    "\n",
    "    return cached_batch_list, text_list       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- ORIGINAL PROMPTS --------\n",
      "id: 0: Pricess Peach jumped from the balcony and\n",
      "id: 1: Mario and Luigi ran out of the store and\n",
      "id: 2: Bowser took out the flamethrower and\n",
      "id: 3: Wario shaved his mustache and\n",
      "id: 4: Toad made a funny sound and\n",
      "\n",
      "\n",
      "-------- COMPLETED GENERATION --------\n",
      "id = 1: Mario and Luigi ran out of the store and back to the park.\n",
      "\n",
      "\n",
      "\n",
      "-------- COMPLETED GENERATION --------\n",
      "id = 0: Pricess Peach jumped from the balcony and landed on the grass. She was so happy to be free.\n",
      "\n",
      "The end.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-------- COMPLETED GENERATION --------\n",
      "id = 3: Wario shaved his mustache and it made him look very handsome. He\n",
      "\n",
      "\n",
      "\n",
      "-------- COMPLETED GENERATION --------\n",
      "id = 2: Bowser took out the flamethrower and said, \"I'm going to give you a special surprise!\"\n",
      "\n",
      "The\n",
      "\n",
      "\n",
      "\n",
      "-------- COMPLETED GENERATION --------\n",
      "id = 4: Toad made a funny sound and hopped away. He was so happy that he\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NUM_DECODES = 5\n",
    "cached_batch_lst = []\n",
    "text_lst = [prompt for prompt in prompts]\n",
    "\n",
    "print(\"-------- ORIGINAL PROMPTS --------\")\n",
    "for idx, prompt in enumerate(prompts):\n",
    "    print(f\"id: {idx}: {prompt}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "for batch in batches:\n",
    "    # prefill\n",
    "    generation, new_cached_batch = service.Prefill(batch)\n",
    "    text_lst[generation.request_id] += generation.token\n",
    "    cached_batch_lst.append(new_cached_batch)\n",
    "\n",
    "    # decodes\n",
    "    for _ in range(NUM_DECODES):\n",
    "        cached_batch_lst, text_lst = do_decode(service, cached_batch_lst, text_lst)\n",
    "\n",
    "# once all the batches have been added\n",
    "for _ in range(100):\n",
    "    cached_batch_lst, text_lst = do_decode(service, cached_batch_lst, text_lst)\n",
    "    if len(cached_batch_lst) == 0:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
